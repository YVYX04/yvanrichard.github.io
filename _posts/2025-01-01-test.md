---
title: Predicting House Prices in Iowa
subtitle: This post presents the typical pipeline a (small) machine learning project follows. I briefly examine    what is the general pipeline used in practice and proceed to develop all the steps of a reduced ML pipeline for
    predicting housing prices in Iowa between 2006 and 2010 with the Ames data set.
layout: default
date: 2025-12-07
keywords: machine learning, logistic regression, random forest
published: true
---

# 1. The Machine Learning Pipeline

In their examination of machine learning workflows at varying scales, Biswas et al. (2022) outline how pipeline structures evolve according to operational requirements. In a professional, production-oriented setting, a fully integrated pipeline typically follows the structure shown below:

<div class='figure'>
    <img src="{{ '/images/DS_pipeline01.png' | relative_url }}" alt="Data pipeline"
         style="width: 100%; display: block; margin: 0 auto;" id = "fig1"/>
    <div class='caption'>
        <span class='caption-label'>Figure 1.</span> Concepts in a data science pipeline. The sub-tasks are listed below each stage. The stages are connected with feedback loops denoted with arrows. Solid arrows are always present in the lifecycle, while the dashed arrows are optional. Distant feedback loops (e.g.,from deployment to data acquisition) are also possible through intermediate stage(s).
    </div>
</div>

Naturally, the structure of a pipeline depends on the scope, constraints, and objectives of the project. **Figure 1** presents a highly comprehensive design that suits large–scale production environments but exceeds what is necessary for exploratory research. For this smaller-scale study, the pipeline is intentionally streamlined to focus on the essential components:

1. Data and Libraries Loading  
2. Exploratory Data Analysis (EDA)  
3. Data Cleaning and Preparation  
4. Feature Engineering  
5. Modeling Stage I  
6. Training and Evaluation  
7. Modeling Stage II (Hyper-parameter Tuning)  
8. Final Predictions  
9. Results and Interpretation  

While the overall structure draws inspiration from the framework presented above, several components are omitted as they are not required in the context of this project. Without further ado, let's dive in the first step!

# 2. Data and Libraries Loading

This project begins by importing a set of essential Python libraries to support the data workflow. `Pandas` is used for cleaning, transforming, and organizing tabular data, while `NumPy` enables efficient numerical operations. `matplotlib` and `seaborn` provide plotting utilities for visualizing distributions, correlations, and model results. Finally, `fetch_openml` from `sklearn.datasets` allows direct access to the Ames Housing dataset from the OpenML repository, making it straightforward to load real-world data for analysis and modeling. I use `tabulate` to generate markdown tables. 

```python
# Libraries
from sklearn.datasets import fetch_openml
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import tabulate

# Ames Housing dataset
ames = fetch_openml(name="house_prices", as_frame=True)

# Display the first few rows of the dataset
df = ames.frame
df.head()
```

Once we properly loaded the libraries relevant to our project, we proceed with the EDA. 

# 3. Exploratory Data Analysis (EDA)

This stage helps us gain a clearer understanding of the dataset’s structure and content. As Behrens (1997, p. 132) states, “the goal of EDA is to discover patterns in data […] until a plausible story of the data is apparent.” In the context of a machine learning project, this is precisely the purpose EDA serves. By examining distributions, interactions, and variability within the data, we build knowledge that shapes the steps that follow. Data cleaning becomes more targeted, allowing decisions about handling missing values to be made with greater justification. Similarly, insights uncovered during EDA inform feature engineering, enabling the creation of features that better account for variance in the target. Ultimately, the strength of our predictions is closely tied to the depth of understanding achieved during this exploratory phase and our ability to translate those findings into meaningful model improvements.

## 3.1. The Ames Housing Data Set and Summary Statistics

### Description of the Ames Housing Data

This data set contains information from the Ames Assessor’s Office used in computing assessed values for individual residential properties sold in Ames (Iowa, US) from 2006 to 2010. This data set is very rich and has a relatively large dimension:

```python
print(df.shape)
```

```text
(1460, 81)
```

This means that we have $1460$ unique observations and roughly $80$ explanatory variable for one target variable named `SalePrice`, that is the price of the house in USD.


### Summary Statistics

#### Nummerical Variables

Behrens (1997, p. 135) importantly said that "for an exploratory data analyst, graphical representation is the primary language". However, I think that we can always start with some summary statistics to get
a very large picture at first glance. First, I compute the summary statistics of the numerical variable with:

```python
stats = df.describe().T
```

and then translate the result to a markdown table presented here.

<p style="text-align:center; font-weight:bold; margin-bottom:10px;">
Table 1 — Summary statistics for the numerical variables in the Ames dataset.
</p>

|               |   count |           mean |          std |   min |       25% |      50% |       75% |    max |
|:--------------|--------:|---------------:|-------------:|------:|----------:|---------:|----------:|-------:|
| Id            |    1460 |    730.5       |   421.61     |     1 |    365.75 |    730.5 |   1095.25 |   1460 |
| MSSubClass    |    1460 |     56.8973    |    42.3006   |    20 |     20    |     50   |     70    |    190 |
| LotFrontage   |    1201 |     70.05      |    24.2848   |    21 |     59    |     69   |     80    |    313 |
| LotArea       |    1460 |  10516.8       |  9981.26     |  1300 |   7553.5  |   9478.5 |  11601.5  | 215245 |
| OverallQual   |    1460 |      6.09932   |     1.383    |     1 |      5    |      6   |      7    |     10 |
| OverallCond   |    1460 |      5.57534   |     1.1128   |     1 |      5    |      5   |      6    |      9 |
| YearBuilt     |    1460 |   1971.27      |    30.2029   |  1872 |   1954    |   1973   |   2000    |   2010 |
| YearRemodAdd  |    1460 |   1984.87      |    20.6454   |  1950 |   1967    |   1994   |   2004    |   2010 |
| MasVnrArea    |    1452 |    103.685     |   181.066    |     0 |      0    |      0   |    166    |   1600 |
| BsmtFinSF1    |    1460 |    443.64      |   456.098    |     0 |      0    |    383.5 |    712.25 |   5644 |
| BsmtFinSF2    |    1460 |     46.5493    |   161.319    |     0 |      0    |      0   |      0    |   1474 |
| BsmtUnfSF     |    1460 |    567.24      |   441.867    |     0 |    223    |    477.5 |    808    |   2336 |
| TotalBsmtSF   |    1460 |   1057.43      |   438.705    |     0 |    795.75 |    991.5 |   1298.25 |   6110 |
| 1stFlrSF      |    1460 |   1162.63      |   386.588    |   334 |    882    |   1087   |   1391.25 |   4692 |
| 2ndFlrSF      |    1460 |    346.992     |   436.528    |     0 |      0    |      0   |    728    |   2065 |
| LowQualFinSF  |    1460 |      5.84452   |    48.6231   |     0 |      0    |      0   |      0    |    572 |
| GrLivArea     |    1460 |   1515.46      |   525.48     |   334 |   1129.5  |   1464   |   1776.75 |   5642 |
| BsmtFullBath  |    1460 |      0.425342  |     0.518911 |     0 |      0    |      0   |      1    |      3 |
| BsmtHalfBath  |    1460 |      0.0575342 |     0.238753 |     0 |      0    |      0   |      0    |      2 |
| FullBath      |    1460 |      1.56507   |     0.550916 |     0 |      1    |      2   |      2    |      3 |
| HalfBath      |    1460 |      0.382877  |     0.502885 |     0 |      0    |      0   |      1    |      2 |
| BedroomAbvGr  |    1460 |      2.86644   |     0.815778 |     0 |      2    |      3   |      3    |      8 |
| KitchenAbvGr  |    1460 |      1.04658   |     0.220338 |     0 |      1    |      1   |      1    |      3 |
| TotRmsAbvGrd  |    1460 |      6.51781   |     1.62539  |     2 |      5    |      6   |      7    |     14 |
| Fireplaces    |    1460 |      0.613014  |     0.644666 |     0 |      0    |      1   |      1    |      3 |
| GarageYrBlt   |    1379 |   1978.51      |    24.6897   |  1900 |   1961    |   1980   |   2002    |   2010 |
| GarageCars    |    1460 |      1.76712   |     0.747315 |     0 |      1    |      2   |      2    |      4 |
| GarageArea    |    1460 |    472.98      |   213.805    |     0 |    334.5  |    480   |    576    |   1418 |
| WoodDeckSF    |    1460 |     94.2445    |   125.339    |     0 |      0    |      0   |    168    |    857 |
| OpenPorchSF   |    1460 |     46.6603    |    66.256    |     0 |      0    |     25   |     68    |    547 |
| EnclosedPorch |    1460 |     21.9541    |    61.1191   |     0 |      0    |      0   |      0    |    552 |
| 3SsnPorch     |    1460 |      3.40959   |    29.3173   |     0 |      0    |      0   |      0    |    508 |
| ScreenPorch   |    1460 |     15.061     |    55.7574   |     0 |      0    |      0   |      0    |    480 |
| PoolArea      |    1460 |      2.7589    |    40.1773   |     0 |      0    |      0   |      0    |    738 |
| MiscVal       |    1460 |     43.489     |   496.123    |     0 |      0    |      0   |      0    |  15500 |
| MoSold        |    1460 |      6.32192   |     2.70363  |     1 |      5    |      6   |      8    |     12 |
| YrSold        |    1460 |   2007.82      |     1.3281   |  2006 |   2007    |   2008   |   2009    |   2010 |
| SalePrice     |    1460 | 180921         | 79442.5      | 34900 | 129975    | 163000   | 214000    | 755000 |

I will not give a description of all the variables here but the interested reader can consult the <a href = "https://jse.amstat.org/v19n3/decock/DataDocumentation.txt">official documentation</a>. It is also interesting to observe the number of missing value for each numerical variable before proceeding further:

```text
Percentage of missing numerical values per variable:
 LotFrontage    17.739726
MasVnrArea      0.547945
GarageYrBlt     5.547945
```

As we can see, only three numerical variables suffer from missing data. However, the question we should also ask ourselves is if all of those *numerical* variables are really numerical (e.g. `Id` is simply a unique identifier).

#### Categorical Variables

I now proceed to compute some statistics about my categorical variables. This work is usually more tedious than when we deal with plain numerical variables. Here is the list of the categorical variables in my data set (so far) :

```python
['MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour',
'Utilities', 'LotConfig', 'LandSlope', 'Neighborhood',
'Condition1', 'Condition2', 'BldgType', 'HouseStyle',
'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd',
'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation',
'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1',
'BsmtFinType2', 'Heating', 'HeatingQC', 'CentralAir',
'Electrical', 'KitchenQual', 'Functional', 'FireplaceQu',
'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond',
'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature',
'SaleType', 'SaleCondition']
```

For a categorical variable, we can typically inspect the unique values encoded by a variable and compute the empirical distribution of these discrete values. The variable `SaleType` will serve as an example. Here is the code
I use to generate an histogram of its empirical distribution:

```python
# retrieve unique values in 'Heating' column and their counts
unique_val = df['SaleType'].value_counts()

# plot the distribution of 'Heating' types
fig, ax = plt.subplots(figsize=(8, 4))
sns.barplot(hue=unique_val.index, y=unique_val.values, ax=ax, palette='viridis')
ax.set_title('Distribution of Sale Types', loc='left', size=12)
ax.set_xlabel('Sale Type')
ax.set_ylabel('Count')
ax.grid(alpha=0.3, axis='y')
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)

# save and show

plt.show()
```

The result is quite minimalist but is immediately informative: most sale contracts are warranty deeds. 
A Warranty Deed (Conventional) is a type of property sale in which the seller guarantees that they hold clear, clean, and transferable title to the real estate and that no outstanding claims, liens, or encumbrances exist other than those already disclosed.

<div class='figure'>
    <img src="{{ '/images/071225_Stype.png' | relative_url }}" alt="Histogram"
         style="width: 100%; display: block; margin: 0 auto;" id = "fig2"/>
    <div class='caption'>
        <span class='caption-label'>Figure 2.</span> This figure depicts the discrete distribution of the different sale types in the ames data set.
    </div>
</div>

**TO BE CONTINUED** 

`last updated on Mon. Dec. 8 at 01:35 UTC`


## References

- Behrens, J. T. (1997). Principles and procedures of exploratory data analysis. *Psychological methods*, 2(2), 131.

- Biswas, S., Wardat, M., & Rajan, H. (2022). The art and practice of data science pipelines: A comprehensive study of data science pipelines in theory, in-the-small, and in-the-large. In *Proceedings of the 44th International Conference on Software Engineering* (pp. 2091-2103).



